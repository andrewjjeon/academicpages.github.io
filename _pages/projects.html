---
layout: archive
title: "Projects"
permalink: /projects/
author_profile: true
---



<h2>Novel Object Robotic Grasping with Foundation Model</h2>

<p>
  FoundationPose is a Unified foundation model for 6D pose estimation and tracking, 
  it has both CAD model-based configurations and model-free configurations.
  Further details can be found in <a href="https://nvlabs.github.io/FoundationPose/" target="_blank">FoundationPose</a>
</p>

<p>
  In order to perform novel object grasping with this model, we propose to run 2x instances of FoundationPose. 
  1x on the robot hand or configuration. 1x on the object to grasp. This should give you the respective pose matrix which 
  contains the 3x1 translation(tx, ty, tz) and 3x3 rotation (r11, r12, r13, r21, r22, r23, ...). At this point,
  you should have the <b>robot-to-camera</b> pose matrix and the <b>object-to-camera</b> pose matrix. If you have these
  two matrices, you can now calculate the <b>robot-to-object</b> pose matrix which in turn would enable robotic grasping.
  The more accurate the pose matrix, the more precise the object grasping with your robot hand or configuration. The
  long-term goal is to make this system robust enough to work with whatever robot model and configuration you have i.e. 
  franka robot hand, franka robot hand + link0, PP100 hand + link0 + link1, and so forth. Our rough approach is illustrated below:
</p>


<div style="text-align: center;">
  <img src="/images/fp_block.JPG" alt="Block Diagram" style="max-width: 100%; height: auto;">
  <p>Block diagram of our proposed approach.</p>
</div>

<div style="text-align: center;">
  <img src="/images/fp_ketchup.gif" alt="FoundationPose object instance" style="max-width: 100%; height: auto;">
  <p>Demonstration of FoundationPose object instance estimating pose and tracking a ketchup bottle from the HOPE Dataset.</p>
</div>




<br><br>
<br><br>




<h2>3DVLMaps: 3D Visual-Language Maps for Robot Navigation</h2>

<p>
  VLMaps is a spatial map representation that embeds pretrained visual-language features with a 3D reconstruction
  and projects to a top-down 2D map. VLMaps embeds visual features from an LSeg visual encoder to points in a point cloud. These points are then projected 
  to a top-down navigation map where only the point with the highest height is kept. After this, the visual features are 
  compared through cosine similarity to textual features from a LSeg text encoder to determine the semantic label of the point. 
  Due to the top-down projection, the robot wouldn't be capable of 3D navigation such as “go to the plant below the table.” 
  
  Further details can be found in <a href="https://vlmaps.github.io/" target="_blank">VLMaps</a>
</p>

<p>
  To address this limitation, I implemented a voxel grid projection by populating the voxel grid cells with their corresponding 
  points. When multiple points fall in the same voxel cell I average all their embeddings to generate a singular embedding. 
  This resulted in successful, 3D semantic segmentation of the voxel grid scene, which would allow a robot to navigate in the 
  3D space.
</p>

<div style="text-align: center;">
  <img src="/images/3dvlmaps_block.jpg" alt="Block Diagram" style="max-width: 100%; height: auto;">
  <p>Block diagram of 3DVLMaps.</p>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/segresult.png" alt="House 3D Segmentation Result" style="max-width: 49%; height: auto;">
    <img src="/images/3dvlmaps_key.png" alt="House 3D Segmentation Key" style="max-width: 49%; height: auto;">
  </div>
  <p>3D segmentation demonstration of household scene.</p>
</div>


<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 10px;">
    <img src="/images/realsofa.png" alt="Real Sofa" style="max-width: 49%; height: auto;">
    <img src="/images/seg_sofa.png" alt="Segmented Sofa" style="max-width: 49%; height: auto;">
  </div>
  <p>3D segmentation demonstration of sofa, comparison with real world point cloud</p>
</div>
