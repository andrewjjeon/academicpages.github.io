---
layout: archive
title: "Projects"
permalink: /projects/
author_profile: true
---



<h2>Novel Object Robotic Grasping with Foundation Model</h2>

<p>FoundationPose is a Unified foundation model for 6D pose estimation and tracking, 
  it has both CAD model-based configurations and model-free configurations.
  For more details, see <a href="https://nvlabs.github.io/FoundationPose/" target="_blank">FoundationPose</a>
</p>

<p>
  In order to perform novel object grasping with this model, we propose to run 2x instances of FoundationPose. 
  1x on the robot hand or configuration. 1x on the object to grasp. This should give you the respective pose matrix which 
  contains the 3x1 translation(tx, ty, tz) and 3x3 rotation (r11, r12, r13, r21, r22, r23, ...). At this point,
  you should have the <b>robot-to-camera</b> pose matrix and the <b>object-to-camera</b> pose matrix. If you have these
  two matrices, you can now calculate the <b>robot-to-object</b> pose matrix which in turn would enable robotic grasping.
  The more accurate the pose matrix, the more precise the object grasping with your robot hand or configuration. The
  long-term goal is to make this system robust enough to work with whatever robot model and configuration you have i.e. 
  franka robot hand, franka robot hand + link0, PP100 hand + link0 + link1, and so forth. Our rough approach is illustrated below:
</p>


<div style="text-align: center;">
  <img src="/images/fp_block.JPG" alt="Block Diagram" style="max-width: 100%; height: auto;">
  <p>Block diagram of our proposed approach.</p>
</div>

<div style="text-align: center;">
  <img src="/images/fp_ketchup.gif" alt="FoundationPose object instance" style="max-width: 100%; height: auto;">
  <p>Demonstration of FoundationPose object instance estimating pose and tracking a ketchup bottle from the HOPE Dataset.</p>
</div>

<br><br>
<br><br>
<br><br>

<h2>3DVLMaps: 3D Visual-Language Maps for Robot Navigation</h2>

